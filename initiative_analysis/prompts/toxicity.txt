Analyze the following text for potentially harmful language:

{{content}}

Toxicity Assessment Criteria:
- Presence of discriminatory language
- Harmful stereotypes or prejudices
- Inflammatory or aggressive tone
- Potential for causing offense

Respond in JSON format:
{{
"toxicity_score": 0.0-1.0,
"toxicity_confidence": 0.0-1.0,
"toxicity_concerns": [
    "Specific toxic language or perspective",
    "Potential harmful implication"
]
}}
